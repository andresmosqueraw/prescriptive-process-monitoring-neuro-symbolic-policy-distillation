# ğŸ¯ GuÃ­a de EjecuciÃ³n: Causal-Gym

## ğŸ“‹ Resumen de Enfoques

Causal-Gym tiene **DOS enfoques** para entrenar la polÃ­tica:

### 1. ğŸš€ Enfoque OFFLINE (Recomendado)
- **Ventajas**: MÃ¡s rÃ¡pido, supera el baseline (+109%), no requiere simulaciÃ³n
- **Script**: `train_from_historical.py`
- **Archivos necesarios**: Solo `bpi2017_train.csv`
- **Archivos generados**: `final_policy_model.pkl`

### 2. ğŸ”„ Enfoque de SIMULACIÃ“N (Alternativo)
- **Ventajas**: Usa simulaciÃ³n realista con Prosimos
- **Scripts**: `extract_bpmn_json.py` â†’ `compute_state.py` â†’ `train_agent_in_gym.py` â†’ `distill_policy.py`
- **Archivos necesarios**: `bpi2017_train.csv` + Docker (para Simod)
- **Archivos generados**: BPMN/JSON, estados parciales, `experience_buffer.csv`, `final_policy_model.pkl`

---

## ğŸ¬ Orden de EjecuciÃ³n Recomendado

### OpciÃ³n A: Enfoque OFFLINE (Por Defecto)

```bash
# Ejecutar el pipeline completo (solo train_from_historical.py)
./scripts/ejecutar-todo.sh
```

**Flujo:**
```
1. extract_bpmn_json.py --train    [OPCIONAL - se omite]
2. compute_state.py --train        [OPCIONAL - se omite]
3. train_from_historical.py        [EJECUTA - entrena desde datos]
```

**Resultado:**
- âœ… Modelo guardado en: `results/bpi2017_train/distill/final_policy_model.pkl`
- âœ… Net Gain esperado: ~$26.56 (vs $12.68 baseline)

---

### OpciÃ³n B: Enfoque de SIMULACIÃ“N

```bash
# Ejecutar con simulaciÃ³n
USE_SIMULATION=true ./scripts/ejecutar-todo.sh
```

**Flujo:**
```
1. extract_bpmn_json.py --train    [EJECUTA - genera BPMN/JSON]
2. compute_state.py --train        [EJECUTA - genera estados parciales]
3. train_agent_in_gym.py           [EJECUTA - entrena RL agent]
4. distill_policy.py                [EJECUTA - destila polÃ­tica]
```

**Resultado:**
- âœ… BPMN/JSON en: `results/bpi2017_train/simod/`
- âœ… Estados en: `results/bpi2017_train/state/`
- âœ… Buffer en: `results/bpi2017_train/rl/experience_buffer.csv`
- âœ… Modelo en: `results/bpi2017_train/distill/final_policy_model.pkl`

---

## ğŸ§ª EvaluaciÃ³n

**Ambos enfoques** se evalÃºan igual:

```bash
# Evaluar el modelo entrenado
python src/benchmark/test_models/test_causal_gym.py --test
```

El script `test_causal_gym.py` detecta automÃ¡ticamente quÃ© tipo de modelo es:
- Si es un **bundle** (offline) â†’ usa `apply_model_policy_offline()`
- Si es un **Pipeline** (simulaciÃ³n) â†’ usa `apply_model_policy()`

---

## ğŸ“Š ComparaciÃ³n de Resultados

| MÃ©trica | Baseline | Causal-Gym (Offline) | Causal-Gym (SimulaciÃ³n) |
|---------|----------|---------------------|------------------------|
| **Net Gain** | $12.68 | **$26.56** (+109%) | Variable |
| **% Intervenciones** | 99.5% | 30.2% | Variable |
| **Tiempo de entrenamiento** | N/A | ~10 segundos | ~30-60 minutos |
| **Requisitos** | N/A | Solo datos CSV | Docker + Simod |

---

## ğŸ”§ EjecuciÃ³n Manual (Si prefieres control total)

### Solo entrenar modelo offline:
```bash
python src/causal-gym/train_from_historical.py
```

### Solo evaluar:
```bash
python src/benchmark/test_models/test_causal_gym.py --test
```

### Pipeline completo de simulaciÃ³n (paso a paso):
```bash
# Paso 1: Extraer BPMN/JSON
python src/causal-gym/extract_bpmn_json.py --train --fast

# Paso 2: Calcular estados parciales
python src/causal-gym/compute_state.py --train

# Paso 3: Entrenar RL agent
python src/causal-gym/train_agent_in_gym.py

# Paso 4: Destilar polÃ­tica
python src/causal-gym/distill_policy.py

# Evaluar
python src/benchmark/test_models/test_causal_gym.py --test
```

---

## â“ Preguntas Frecuentes

**Q: Â¿CuÃ¡l enfoque debo usar?**  
A: **Offline** es recomendado porque es mÃ¡s rÃ¡pido y supera el baseline. Usa simulaciÃ³n solo si necesitas validar el comportamiento en un entorno simulado.

**Q: Â¿Puedo ejecutar ambos enfoques?**  
A: SÃ­, pero generarÃ¡n modelos diferentes. El Ãºltimo modelo guardado serÃ¡ el que use `test_causal_gym.py`.

**Q: Â¿Los pasos 1 y 2 son necesarios para offline?**  
A: No. El enfoque offline solo necesita `bpi2017_train.csv`.

**Q: Â¿CÃ³mo sÃ© quÃ© modelo se estÃ¡ usando?**  
A: `test_causal_gym.py` detecta automÃ¡ticamente el tipo de modelo y muestra un mensaje en los logs.

---

## ğŸ“ Notas TÃ©cnicas

- El modelo offline usa **predicciÃ³n de outcome** para decidir intervenciones
- El modelo de simulaciÃ³n usa **RL con recompensas causales** en Prosimos
- Ambos modelos se guardan en el mismo directorio pero con diferentes estructuras internas
- `test_causal_gym.py` es compatible con ambos tipos de modelos


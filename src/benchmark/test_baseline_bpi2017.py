#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script para probar el Benchmark Evaluator con el dataset BPI Challenge 2017.
Calcula las mÃ©tricas del baseline (Business As Usual).
"""

import os
import sys
import yaml
import pandas as pd
import numpy as np
from typing import Dict, Any, Optional

# Agregar src/ al PYTHONPATH para encontrar utils
script_dir = os.path.dirname(os.path.abspath(__file__))
src_dir = os.path.dirname(script_dir)  # src/
if src_dir not in sys.path:
    sys.path.insert(0, src_dir)

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

from benchmark_evaluator import BenchmarkEvaluator
from utils.config import load_config
from utils.logger_utils import setup_logger

# Configurar logger
logger = setup_logger(__name__)

def load_benchmark_config(config_path: Optional[str] = None) -> Optional[Dict[str, Any]]:
    """
    Carga la configuraciÃ³n del benchmark desde benchmark_config.yaml.
    """
    if config_path is None:
        # Determinar directorio base del proyecto
        script_dir = os.path.dirname(os.path.abspath(__file__))
        src_dir = os.path.dirname(script_dir)  # src/
        project_root = os.path.dirname(src_dir)  # proyecto raÃ­z
        config_path = os.path.join(project_root, "configs", "benchmark_config.yaml")
    
    if not os.path.exists(config_path):
        return None
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config
    except Exception as e:
        logger.warning(f"Error cargando configuraciÃ³n del benchmark: {e}")
        return None

def load_bpi2017_data(csv_path: str) -> pd.DataFrame:
    logger.info(f"Cargando datos desde: {csv_path}")
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"No se encontrÃ³ el archivo: {csv_path}")
    try:
        df = pd.read_csv(csv_path, low_memory=False)
        logger.info(f"Datos cargados: {len(df)} filas, {len(df.columns)} columnas")
        return df
    except Exception as e:
        logger.error(f"Error leyendo CSV: {e}")
        raise

def estimate_propensity_score(df_cases: pd.DataFrame) -> np.ndarray:
    logger.info("Estimando Propensity Scores reales (Sin Data Leakage)...")
    feature_cols = []
    if 'num_events' in df_cases.columns: feature_cols.append('num_events')
    if 'duration_days' in df_cases.columns: feature_cols.append('duration_days')
    
    if not feature_cols:
        logger.warning("No features, usando promedio.")
        return np.full(len(df_cases), df_cases['treatment_observed'].mean())
    
    features = df_cases[feature_cols].fillna(0)
    target = df_cases['treatment_observed']

    # Check si hay al menos 2 clases
    if len(np.unique(target)) < 2:
        logger.warning(f"Solo hay una clase en target ({np.unique(target)}). Usando probabilidad promedio constante.")
        # Si todos son 1, PS = 0.99. Si todos son 0, PS = 0.01.
        mean_val = np.mean(target)
        ps_val = 0.99 if mean_val > 0.5 else 0.01
        return np.full(len(df_cases), ps_val)
    
    try:
        clf = LogisticRegression(class_weight='balanced', random_state=42)
        clf.fit(features, target)
        ps = clf.predict_proba(features)[:, 1]
        return np.clip(ps, 0.05, 0.95)
    except Exception as e:
        logger.warning(f"Error estimando PS: {e}")
        return np.full(len(df_cases), target.mean())

def prepare_baseline_dataframe(df_events: pd.DataFrame) -> pd.DataFrame:
    logger.info("Preparando DataFrame (LÃ³gica Estricta BPI 2017)...")
    
    # Mapeo robusto
    column_mapping = {
        'case_id': 'case:concept:name',
        'activity': 'concept:name',
        'timestamp': 'time:timestamp'
    }
    for k, v in column_mapping.items():
        if v not in df_events.columns:
            if k == 'case_id': column_mapping[k] = df_events.columns[0]
            if k == 'activity': column_mapping[k] = 'Activity' if 'Activity' in df_events.columns else df_events.columns[1]
            if k == 'timestamp': column_mapping[k] = 'time:timestamp'
            
    case_col = column_mapping['case_id']
    act_col = column_mapping['activity']
    time_col = column_mapping['timestamp']
    
    # AgrupaciÃ³n
    if time_col in df_events.columns:
        df_events[time_col] = pd.to_datetime(df_events[time_col], utc=True, format='mixed', errors='coerce')
        
    df_cases = df_events.groupby(case_col).agg(
        start_time=(time_col, 'min'),
        end_time=(time_col, 'max'),
        num_events=(act_col, 'count')
    ).reset_index()
    
    df_cases['duration_days'] = (df_cases['end_time'] - df_cases['start_time']).dt.total_seconds() / (24 * 3600)
    
    # Tratamiento (Exact Match)
    treatment_exact_match = ['W_Call after offers', 'W_Call incomplete files']
    mask_treatment = df_events[act_col].isin(treatment_exact_match)
    
    if mask_treatment.sum() == 0:
        mask_treatment = df_events[act_col].astype(str).str.contains('W_Call', case=False, na=False)
        
    treated_case_ids = df_events.loc[mask_treatment, case_col].unique()
    df_cases['treatment_observed'] = df_cases[case_col].isin(treated_case_ids).astype(int)
    
    # Outcome
    success_activities = ['O_Accepted']
    success_case_ids = df_events[df_events[act_col].isin(success_activities)][case_col].unique()
    df_cases['outcome_observed'] = df_cases[case_col].isin(success_case_ids).astype(int)
    
    # Propensity Score & Config Baseline
    try:
        df_cases['propensity_score'] = estimate_propensity_score(df_cases)
    except Exception:
        df_cases['propensity_score'] = df_cases['treatment_observed'].mean()

    df_cases['action_model'] = df_cases['treatment_observed']
    df_cases['current_state'] = 'Closed'
    df_cases['days_since_last_intervention'] = 999
    df_cases['uplift_score'] = None
    
    df_cases = df_cases.rename(columns={case_col: 'case_id'})
    
    return df_cases[['case_id', 'outcome_observed', 'treatment_observed', 'duration_days', 
                     'action_model', 'propensity_score', 'uplift_score', 'current_state', 
                     'days_since_last_intervention']]

def process_baseline_for_log(csv_path: str, log_name: str, project_root: str) -> None:
    logger.info(f"\n{'='*80}\nPROCESANDO BASELINE: {log_name}\n{'='*80}")
    
    try:
        df_events = load_bpi2017_data(csv_path)
        df_results = prepare_baseline_dataframe(df_events)
    except Exception as e:
        logger.error(f"Error procesando {log_name}: {e}")
        return
    
    evaluator = BenchmarkEvaluator()
    
    # 1. MÃ©tricas estÃ¡ndar
    results = evaluator.evaluate(df_results, training_complexity="N/A (Baseline)")
    
    # 2. Verdad HistÃ³rica (Ground Truth)
    historical_gain = evaluator.calculate_historical_net_gain(df_results)
    results['net_gain'] = historical_gain
    results['lift_vs_bau'] = 0.0
    results['latency_ms'] = 0.0
    
    logger.info(f"\nRESULTADOS OFICIALES ({log_name}):")
    logger.info(f"ðŸ’° Net Gain: ${results['net_gain']:.2f}")
    logger.info(f"ðŸ“‰ Intervenciones: {results.get('intervention_percentage', 0):.2f}%")
    
    # Guardar
    benchmark_config = load_benchmark_config()
    base_dir = benchmark_config.get("output", {}).get("base_dir", "results/benchmark") if benchmark_config else "results/benchmark"
    metrics_filename = benchmark_config.get("output", {}).get("metrics_filename", "baseline_metrics.csv") if benchmark_config else "baseline_metrics.csv"
    
    if not os.path.isabs(base_dir):
        base_dir = os.path.join(project_root, base_dir)
    
    output_dir = os.path.join(base_dir, log_name)
    os.makedirs(output_dir, exist_ok=True)
    pd.DataFrame([results]).to_csv(os.path.join(output_dir, metrics_filename), index=False)
    logger.info(f"âœ… Guardado en: {os.path.join(output_dir, metrics_filename)}")

def main() -> None:
    logger.info(f"{'='*80}\nTEST BASELINE - BPI CHALLENGE 2017\n{'='*80}")
    
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(os.path.dirname(script_dir))
    
    benchmark_config = load_benchmark_config()
    logs_to_process = []
    
    if benchmark_config:
        logs_conf = benchmark_config.get("logs", {})
        if benchmark_config.get("processing", {}).get("process_both_logs", True):
            if logs_conf.get("full_log", {}).get("csv_path"): 
                path = logs_conf["full_log"]["csv_path"]
                if not os.path.isabs(path): path = os.path.join(project_root, path)
                logs_to_process.append(path)
            if logs_conf.get("sample_log", {}).get("csv_path"):
                path = logs_conf["sample_log"]["csv_path"]
                if not os.path.isabs(path): path = os.path.join(project_root, path)
                logs_to_process.append(path)
    
    if not logs_to_process:
        # Fallback
        default_full = os.path.join(project_root, "logs/BPI2017/bpi-challenge-2017.csv")
        if os.path.exists(default_full): logs_to_process.append(default_full)
        
    for csv_path in logs_to_process:
        if os.path.exists(csv_path):
            log_name = os.path.splitext(os.path.basename(csv_path))[0].replace(".csv", "").replace(".xes", "")
            process_baseline_for_log(csv_path, log_name, project_root)
        else:
            logger.warning(f"Log no encontrado: {csv_path}")

if __name__ == "__main__":
    main()